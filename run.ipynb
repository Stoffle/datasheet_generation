{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Tuple, Hashable, Dict, Set, cast\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "import warnings\n",
    "\n",
    "from baynet import DAG, metrics\n",
    "\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "from cdt.causality import graph\n",
    "from cdt.utils import dagify_min_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cdt_to_baynet(cpdag: nx.DiGraph, columns: List[str]) -> DAG:\n",
    "    adj_matrix = np.array(nx.adj_matrix(cpdag).todense())\n",
    "    graph = DAG.from_amat(adj_matrix, columns)\n",
    "    return graph\n",
    "\n",
    "def PC(data: pd.DataFrame, ci_test: str = \"discrete\") -> DAG:\n",
    "    \"\"\"\n",
    "    Wrap of the CDT predict function of the PC algorithm. Return a learnt DAG from data.\n",
    "\n",
    "    :param data: The data which the structure will be learnt with (Pandas.DataFrame)\n",
    "    :param ci_test: The (str) argument specifying which Conditional Indep. test to use\n",
    "    :return: A learnt DAG (BayNet DAG)\n",
    "    \"\"\"\n",
    "    pc_alg = graph.PC(CItest=ci_test, verbose=False)\n",
    "    cpdag = pc_alg.predict(data)\n",
    "    dag = dagify_min_edge(cpdag)\n",
    "    return _cdt_to_baynet(dag, list(data.columns))\n",
    "\n",
    "def GES(data: pd.DataFrame, score: str = \"int\") -> DAG:\n",
    "    \"\"\"\n",
    "    Wrap of the CDT predict function of the GES algorithm. Return a learnt DAG from data.\n",
    "\n",
    "    :param data: The data which the structure will be learnt with (Pandas.DataFrame)\n",
    "    :param score: The (str) argument specifying which score function to use\n",
    "    :return: A learnt DAG (BayNet DAG)\n",
    "    \"\"\"\n",
    "    ges_alg = graph.GES(score=score, verbose=False)\n",
    "    cpdag = ges_alg.predict(data)\n",
    "    dag = dagify_min_edge(cpdag)\n",
    "    return _cdt_to_baynet(dag, list(data.columns))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_combinations_odds_ratio(\n",
    "    model: DAG,\n",
    "    target: Optional[str] = None,\n",
    "    data: Optional[pd.DataFrame] = None,\n",
    ") -> Tuple[dict, str]:\n",
    "    \"\"\"\n",
    "    Get a dictionary object containing all possible odds ratios on a target.\n",
    "\n",
    "    :param model: A DAG object on which to calculate the odds ratios\n",
    "    :param target: A str specifying the target node\n",
    "    :return: A dictionary of odds ratios, with tuple keys\n",
    "    \"\"\"\n",
    "    odds_ratio_dict = dict()\n",
    "\n",
    "    if target is not None and target not in list(map(str, model.nodes)):\n",
    "        raise ValueError(f'\"{target}\" node not found in model.')\n",
    "\n",
    "    if target is None:\n",
    "        target = get_target_variable(model)\n",
    "\n",
    "    target_levels = model.get_node(target)[\"levels\"]\n",
    "    target_reference = target_levels[0]\n",
    "\n",
    "    valid_nodes = (\n",
    "        model.nodes - {target}\n",
    "    )\n",
    "    for target_target in target_levels[1:]:\n",
    "        for evidence in valid_nodes:\n",
    "            evidence_levels = model.get_node(evidence)[\"levels\"]\n",
    "            evidence_reference = evidence_levels[0]\n",
    "            for evidence_target in evidence_levels[1:]:\n",
    "                or_key = (\n",
    "                    target,\n",
    "                    str(target_reference),\n",
    "                    str(target_target),\n",
    "                    evidence,\n",
    "                    str(evidence_reference),\n",
    "                    str(evidence_target),\n",
    "                )\n",
    "                or_result = odds_ratio(\n",
    "                    model,\n",
    "                    target,\n",
    "                    evidence,\n",
    "                    target_reference,\n",
    "                    target_target,\n",
    "                    evidence_reference,\n",
    "                    evidence_target,\n",
    "                    data=data,\n",
    "                )\n",
    "                if or_result is not None:\n",
    "                    odds_ratio_dict[or_key] = or_result\n",
    "\n",
    "    return odds_ratio_dict, target\n",
    "\n",
    "def get_leaves(bayesian_network: DAG) -> Set[str]:\n",
    "    return {\n",
    "        node[\"name\"]\n",
    "        for node in bayesian_network.vs\n",
    "        if bayesian_network.degree(node, mode=\"out\") == 0\n",
    "    }\n",
    "\n",
    "def get_target_variable(bayesian_network: DAG) -> str:\n",
    "    \"\"\"Get a target variable.\"\"\"\n",
    "    # Get number of ancestors for each leaf node\n",
    "    target_candidates = {\n",
    "        n: len(list(bayesian_network.predecessors(n))) for n in get_leaves(bayesian_network)\n",
    "    }\n",
    "\n",
    "    # Use node with highest ancestor count\n",
    "    return str(max(target_candidates, key=target_candidates.get))\n",
    "\n",
    "def _get_resolution_order(model: DAG) -> List[str]:\n",
    "    nx_model = nx.DiGraph()\n",
    "    nx_model.add_nodes_from(list(model.nodes))\n",
    "    nx_model.add_edges_from(list(model.edges))\n",
    "    dependency_order = nx.topological_sort(nx_model)\n",
    "    return list(dependency_order)\n",
    "\n",
    "def odds_ratio(\n",
    "    model: DAG,\n",
    "    target: str,\n",
    "    evidence: str,\n",
    "    target_reference_level: str,\n",
    "    target_target_level: str,\n",
    "    evidence_reference_level: str,\n",
    "    evidence_target_level: str,\n",
    "    data: Optional[pd.DataFrame] = None,\n",
    ") -> Tuple[np.float64, np.float64, np.float64]:\n",
    "    \"\"\"\n",
    "    Produce odds ratios for a target node given some evidence.\n",
    "\n",
    "    Does so via CPD manipulation/propagation, as opposed to network sampling.\n",
    "\n",
    "    :param data:\n",
    "    :param model: DAG with structure and CPDs defined.\n",
    "    :param target: The target node to estimate the effect on\n",
    "    :param target_target_level: The target level of the target node\n",
    "    :param target_reference_level: The reference level of the target node\n",
    "    :param evidence: The node which is being intervened on (evidence provided for)\n",
    "    :param evidence_target_level: The target level for the evidence node\n",
    "    :param evidence_reference_level: The reference level for the evidence node\n",
    "\n",
    "    :return: the estimated odds ratio\n",
    "    :type: numpy.float64\n",
    "    \"\"\"\n",
    "    #  If the evidence node is a direct descendent of the target node then\n",
    "    #  intervening on it will remove the latter from the graph.\n",
    "    if evidence in model.get_descendants(target, only_children=True)[\"name\"]:\n",
    "        warnings.warn(\n",
    "            f\"Odds ratio cannot be computed as evidence node '{evidence}' is a direct descendent \"\n",
    "            f\"of target node '{target}'\"\n",
    "        )\n",
    "        dummy = cast(np.float64, 1)\n",
    "        return dummy, dummy, dummy\n",
    "    target_reference_idx = int(target_reference_level)\n",
    "    target_target_idx = int(target_target_level)\n",
    "    return _static_intervention(\n",
    "        model=model,\n",
    "        target=target,\n",
    "        evidence=evidence,\n",
    "        evidence_level=str(evidence_reference_level),\n",
    "        target_level=str(evidence_target_level),\n",
    "        target_target_idx=target_target_idx,\n",
    "        target_reference_idx=target_reference_idx,\n",
    "        data=data,\n",
    "    )\n",
    "    \n",
    "def _static_intervention(\n",
    "    model: DAG,\n",
    "    target: str,\n",
    "    evidence: str,\n",
    "    evidence_level: Hashable,\n",
    "    target_level: Hashable,\n",
    "    target_target_idx: int,\n",
    "    target_reference_idx: int,\n",
    "    data: Optional[pd.DataFrame],\n",
    ") -> Tuple[np.float64, np.float64, np.float64]:\n",
    "    \"\"\"\n",
    "    Get distribution of the `target` node after intervening on the evidence node's evidence_level.\n",
    "\n",
    "    :param model: DAG to perform intervention on; with structure and CPDs defined.\n",
    "    :param target: node to assess interventional outcome\n",
    "    :param evidence: node to perform intervention on\n",
    "    :param evidence_level: level of `evidence` node to perform intervention on\n",
    "    :return: resultant distribution of the `target` node after propagating intervention.\n",
    "    \"\"\"\n",
    "    if evidence not in model.nodes:\n",
    "        return 1, 1, 1\n",
    "    reference_model = model.mutilate(evidence, str(evidence_level))\n",
    "    target_model = model.mutilate(evidence, str(target_level))\n",
    "    n_data = 1 if data is None else len(data)\n",
    "    target_dist = _propogate_probability(target_model, target) * n_data\n",
    "    reference_dist = _propogate_probability(reference_model, target) * n_data\n",
    "    result = (target_dist[target_target_idx] / target_dist[target_reference_idx]) / (\n",
    "        reference_dist[target_target_idx] / reference_dist[target_reference_idx]\n",
    "    )\n",
    "    standard_error = np.sqrt(\n",
    "        1 / target_dist[target_target_idx]\n",
    "        + 1 / target_dist[target_reference_idx]\n",
    "        + 1 / reference_dist[target_target_idx]\n",
    "        + 1 / reference_dist[target_reference_idx]\n",
    "    )\n",
    "    upper = np.exp(np.log(result) + 1.96 * standard_error)\n",
    "    lower = np.exp(np.log(result) - 1.96 * standard_error)\n",
    "    if data is None:\n",
    "        return result, result, result\n",
    "    return result, upper, lower\n",
    "\n",
    "def _propogate_probability(model: DAG, target: str) -> np.ndarray:\n",
    "    accumulated_probs: Dict[str, np.ndarray] = dict()\n",
    "\n",
    "    for node in _get_resolution_order(model):\n",
    "\n",
    "        # Get CPD of curr node\n",
    "        node_cpd = model.get_node(node)[\"CPD\"].array\n",
    "        ordered_parent_nodes = model.get_node(node)[\"CPD\"].parents\n",
    "\n",
    "        # Init result distribution\n",
    "        result_dist = node_cpd\n",
    "\n",
    "        # If node has parents, perform accumulation step\n",
    "        if ordered_parent_nodes:\n",
    "            for parent_node in reversed(ordered_parent_nodes):\n",
    "                result_dist = accumulated_probs[parent_node].dot(result_dist)\n",
    "\n",
    "        accumulated_probs[node] = result_dist\n",
    "    return accumulated_probs[target]\n",
    "\n",
    "def _classify_or(odds_ratio_result: Tuple[float, float, float]) -> str:\n",
    "    odds_ratio, upper, lower = odds_ratio_result\n",
    "    if odds_ratio > 1:\n",
    "        if upper > 1 and lower > 1:\n",
    "            return \"D\"\n",
    "    if odds_ratio < 1:\n",
    "        if upper < 1 and lower < 1:\n",
    "            return \"P\"\n",
    "    return \"N\"\n",
    "\n",
    "def _pcor(\n",
    "    true_ors: Dict[str, Tuple[float, float, float]],\n",
    "    learnt_ors: Dict[str, Tuple[float, float, float]],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the PCOR (prop of correct odds ratios) given two dicts of odds ratios.\n",
    "\n",
    "    @param true_ors: Dictionary of true odds ratios (or_key, or)\n",
    "    @param learnt_ors: Dictionary of learnt odds ratios (or_key, or)\n",
    "\n",
    "    @return: the proportion of correct odds ratios\n",
    "    \"\"\"\n",
    "    hamming = 0\n",
    "    for k, v in true_ors.items():\n",
    "        true_or_class = _classify_or(v)\n",
    "        learnt_or_class = _classify_or(learnt_ors[k])\n",
    "        if true_or_class != learnt_or_class:\n",
    "            hamming += 1\n",
    "    return 1 - (hamming / len(true_ors.keys()))\n",
    "\n",
    "\n",
    "def calculate_pcor(true_bn: DAG, learnt_bn: DAG):\n",
    "    true_ors, target = all_combinations_odds_ratio(true_bn)\n",
    "    learnt_ors, _ = all_combinations_odds_ratio(learnt_bn, target)\n",
    "    return _pcor(true_ors=true_ors, learnt_ors=learnt_ors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = list(range(1, 11)) # 1 -> 10\n",
    "structure_types = [\"forest_fire\", \"barabasi_albert\", \"watts_strogatz\", \"ide_cozman\", \"waxman\"] \n",
    "variables = [40]\n",
    "samples = [5000]\n",
    "alphas = [6.0]\n",
    "max_levels = [4]\n",
    "\n",
    "algorithms = [GES, PC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cab82eef024b71b101e1407fdca98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tq.tqdm._instances.clear()\n",
    "\n",
    "columns = [\"Trial\", \"Structure Type\", \"N_Variables\",\n",
    "           \"N_Samples\", \"Alpha\", \"Max_Level\",\n",
    "           \"Algorithm\", \"Skeleton Precision\", \"Skeleton Recall\", \n",
    "           \"V_Structure_Precision\", \"V_Structure_Recall\", \"PCOR\"]\n",
    "results = []\n",
    "\n",
    "for trial, structure_type, variable, sample, alpha, max_level, algorithm in tq.tqdm(list(product(*[trials, structure_types, variables, samples, alphas, max_levels, algorithms]))):\n",
    "    # ---------  Create Data ------------\n",
    "    dag = DAG.generate(structure_type, variable)\n",
    "    dag.generate_discrete_parameters(alpha=alpha, min_levels=2, max_levels=max_level, seed=trial)\n",
    "    data = dag.sample(sample)\n",
    "    # --------- Learn BN ---------------\n",
    "    # Learn Structure\n",
    "    learnt_dag = algorithm(data)\n",
    "    # Learn Parameters\n",
    "    learnt_dag.estimate_parameters(data, infer_levels=True)\n",
    "    # -------- Calculate Metrics -------------\n",
    "    # Skeleton\n",
    "    precision = metrics.precision(dag, learnt_dag, skeleton=True)\n",
    "    recall = metrics.recall(dag, learnt_dag, skeleton=True)\n",
    "    # V Structures\n",
    "    v_precision = metrics.v_precision(dag, learnt_dag)\n",
    "    v_recall = metrics.v_recall(dag, learnt_dag)\n",
    "    # PCOR\n",
    "    pcor = calculate_pcor(dag, learnt_dag)\n",
    "    results.append([trial, structure_type, variable, sample, alpha, max_level, algorithm.__name__, precision, recall, v_precision, v_recall, pcor])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "results_df.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial</th>\n",
       "      <th>Structure Type</th>\n",
       "      <th>N_Variables</th>\n",
       "      <th>N_Samples</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Max_Level</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Skeleton Precision</th>\n",
       "      <th>Skeleton Recall</th>\n",
       "      <th>V_Structure_Precision</th>\n",
       "      <th>V_Structure_Recall</th>\n",
       "      <th>PCOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>forest_fire</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>GES</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.358108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>forest_fire</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>PC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.168539</td>\n",
       "      <td>0.020270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>barabasi_albert</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>GES</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.108108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>barabasi_albert</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>PC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>watts_strogatz</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>GES</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.100592</td>\n",
       "      <td>0.309091</td>\n",
       "      <td>0.283784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>10</td>\n",
       "      <td>watts_strogatz</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>PC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.406667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>10</td>\n",
       "      <td>ide_cozman</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>GES</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.506667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10</td>\n",
       "      <td>ide_cozman</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>PC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.881579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10</td>\n",
       "      <td>waxman</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>GES</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.606667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10</td>\n",
       "      <td>waxman</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Trial   Structure Type  N_Variables  N_Samples  Alpha  Max_Level  \\\n",
       "0       1      forest_fire           40       5000    6.0          4   \n",
       "1       1      forest_fire           40       5000    6.0          4   \n",
       "2       1  barabasi_albert           40       5000    6.0          4   \n",
       "3       1  barabasi_albert           40       5000    6.0          4   \n",
       "4       1   watts_strogatz           40       5000    6.0          4   \n",
       "..    ...              ...          ...        ...    ...        ...   \n",
       "95     10   watts_strogatz           40       5000    6.0          4   \n",
       "96     10       ide_cozman           40       5000    6.0          4   \n",
       "97     10       ide_cozman           40       5000    6.0          4   \n",
       "98     10           waxman           40       5000    6.0          4   \n",
       "99     10           waxman           40       5000    6.0          4   \n",
       "\n",
       "   Algorithm  Skeleton Precision  Skeleton Recall  V_Structure_Precision  \\\n",
       "0        GES            0.812500         0.764706               0.538462   \n",
       "1         PC            1.000000         0.631579               1.000000   \n",
       "2        GES            0.771429         0.692308               0.450000   \n",
       "3         PC            1.000000         0.589744               1.000000   \n",
       "4        GES            0.444444         0.800000               0.100592   \n",
       "..       ...                 ...              ...                    ...   \n",
       "95        PC            1.000000         0.675000               0.709677   \n",
       "96       GES            0.579545         0.927273               0.132353   \n",
       "97        PC            1.000000         0.961538               0.850000   \n",
       "98       GES            0.483516         0.758621               0.119266   \n",
       "99        PC            0.980769         0.645570               0.500000   \n",
       "\n",
       "    V_Structure_Recall      PCOR  \n",
       "0             0.328125  0.358108  \n",
       "1             0.168539  0.020270  \n",
       "2             0.157895  0.108108  \n",
       "3             0.142857  0.027027  \n",
       "4             0.309091  0.283784  \n",
       "..                 ...       ...  \n",
       "95            0.407407  0.406667  \n",
       "96            0.321429  0.506667  \n",
       "97            0.894737  0.881579  \n",
       "98            0.228070  0.606667  \n",
       "99            0.170732  0.600000  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
